{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os, sys, keras\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from graphTools import *\n",
    "from layerTools import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Header One-Hot Map: {8.0: array([0., 0., 0., 1., 0.]), 1.0: array([1., 0., 0., 0., 0.]), 9.0: array([0., 0., 0., 0., 1.]), 6.0: array([0., 1., 0., 0., 0.]), 7.0: array([0., 0., 1., 0., 0.])}\n",
      "Bond One-Hot Map: {1.0: array([1., 0., 0.]), 2.0: array([0., 1., 0.]), 3.0: array([0., 0., 1.])}\n",
      "Graph Size: 29\n",
      "Number of dropped graphs: 0\n",
      "(150, 29, 5)\n",
      "(150, 29, 29, 3)\n",
      "(150, 29, 29)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Read in graphs\n",
    "graph_files = glob.glob('qm9graph/*.csv')[0:150]\n",
    "mygraphs = [readgraph(fname) for fname in graph_files]\n",
    "headers_onehot_dict, matrices_onehot_dict, mygraphs_standardized = standardize_graphs(mygraphs, max_size=29)\n",
    "headers_padded = np.array([i['header'] for i in mygraphs_standardized])\n",
    "matrices_padded = np.array([i['matrix'] for i in mygraphs_standardized])\n",
    "connectivities_padded = np.array([i['connectivity'] for i in mygraphs_standardized])\n",
    "origHeaders_padded = np.array([i['origHeader'] for i in mygraphs_standardized])\n",
    "origMatrices_padded = np.array([i['origMatrix'] for i in mygraphs_standardized])\n",
    "print headers_padded.shape\n",
    "print matrices_padded.shape\n",
    "print connectivities_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generator\n",
    "\n",
    "def image_categorical_crossentropy(y_true, y_pred):\n",
    "    # Boilerplate off the internet to do crossentropy for multi-classification tasks,\n",
    "    # in this case assigning one-hot atom labels to atoms\n",
    "    __EPS = 1e-5\n",
    "    y_pred = K.clip(y_pred, __EPS, 1 - __EPS)\n",
    "    return -K.mean(y_true * K.log(y_pred) + (1 - y_true) * K.log(1 - y_pred))\n",
    "    \n",
    "def gen_generator_model(num_nodes=29, input_shape=(1,), BO_dense_resize=200, BO_softmax_resize=4, labelator=None, labelatorTrainable=False):\n",
    "    # Creates a Generator model. I chose the architecture arbitrarily\n",
    "    # The architecture is random vector -> dense BO matrix -> softmax to dense 3D BO matrix\n",
    "    # -> symmetrize -> dense -> dense to 29x29x6 matrix.\n",
    "    \n",
    "    inputLayer = keras.layers.Input(shape=input_shape)\n",
    "\n",
    "    output__ = keras.layers.Dense(num_nodes*num_nodes, activation='relu')(inputLayer)\n",
    "    output_ = keras.layers.Reshape((num_nodes,num_nodes))(output__)\n",
    "    output_1 = keras.layers.Lambda(lambda x: K.expand_dims(x + K.permute_dimensions(x,(0,2,1)), axis=3))(output_)\n",
    "    output_1_interpret = keras.layers.Dense(BO_dense_resize)(output_1)\n",
    "    #print output_1\n",
    "    #output_as_BO_mat = keras.layers.Dense(BO_dense_resize, activation='relu')(output_1)\n",
    "    #output_as_BO_mat_stacked = keras.layers.Lambda(lambda x: K.concatenate([x, K.permute_dimensions(x,(0,2,1,3))], axis=3))(output_as_BO_mat)\n",
    "    \n",
    "    BO_mat_softmax_ = keras.layers.Dense(BO_softmax_resize, activation='softmax')(output_1_interpret)\n",
    "    \n",
    "    # Symmetrize the BO matrix\n",
    "    BO_mat_softmax = keras.layers.Lambda(lambda x: (x+K.permute_dimensions(x,(0,2,1,3)))/2.0)(BO_mat_softmax_)\n",
    "    \n",
    "    connectivity = keras.layers.Lambda(lambda x: K.sum(x[:,:,:,1:], axis=3))(BO_mat_softmax)\n",
    "    assert len(inputLayer.shape) == 2, \"Unhandled Input Layer size.\"\n",
    "\n",
    "    annotations__ = keras.layers.Dense(num_nodes)(inputLayer)\n",
    "    annotations_ = keras.layers.Reshape((num_nodes,1))(annotations__)\n",
    "    annotations = keras.layers.Dense(6, activation='softmax')(annotations_)\n",
    "    if labelator == 'default':\n",
    "        pass\n",
    "    else:\n",
    "        # Labelator is a pretrained network (legacy code)\n",
    "        if labelator == None:\n",
    "            annotations = keras.layers.Lambda(lambda x: x*0)(annotations)\n",
    "            annotations.trainable = False\n",
    "        else:\n",
    "            # Labelator is an NN.\n",
    "            labelator.trainable = labelatorTrainable\n",
    "            annotations = labelator([BO_mat_softmax, annotations, connectivity])[1]\n",
    "    model = keras.models.Model(inputs=inputLayer, outputs=[BO_mat_softmax, annotations, connectivity])\n",
    "    return model\n",
    "\n",
    "generator_input = keras.layers.Input(shape=(15,))\n",
    "#bond_hiddens_input = keras.layers.Input(shape=(num_nodes,num_nodes,bond_hidden_length))\n",
    "#atom_hiddens_input = keras.layers.Input(shape=(num_nodes,atom_hidden_length))\n",
    "#connectivity_input = keras.layers.Input(shape=(num_nodes,num_nodes))\n",
    "\n",
    "\n",
    "# Construct generator portion of GAN\n",
    "myGen_intermediate = gen_generator_model(num_nodes=29, input_shape=(15,), BO_dense_resize=30, BO_softmax_resize=4,\\\n",
    "                            labelator=None, labelatorTrainable=False)\n",
    "myGen_conv_1 = generate_gc_model(num_nodes=29, atom_hidden_length=6, bond_hidden_length=4, hide_atoms=False,\\\n",
    "                      message_dense_resize=30, atom_dense_resize=30, bond_dense_resize=1000, do_readout = False)\n",
    "\n",
    "myGen_intermediate_output = myGen_intermediate(generator_input)\n",
    "myGen_output_1 = myGen_conv_1(myGen_intermediate_output)\n",
    "#myGen_output_2 = myGen_conv_1(myGen_output_1)\n",
    "myGen = keras.models.Model(inputs=generator_input, outputs=myGen_output_1)\n",
    "\n",
    "# Construct discriminator portion of GAN\n",
    "myDisc = generate_gc_model(num_nodes=29, atom_hidden_length=6, bond_hidden_length=4, hide_atoms=False,\\\n",
    "                      message_dense_resize=30, atom_dense_resize=30, bond_dense_resize=None, do_readout = True)\n",
    "\n",
    "myDisc.compile(optimizer=keras.optimizers.Adam(lr=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['mean_squared_error', 'mean_absolute_error','binary_crossentropy'])\n",
    "\n",
    "# Hook everything up together\n",
    "myGenOutput = myGen(generator_input)\n",
    "myDiscOutput = myDisc(myGenOutput)\n",
    "myBigModel = keras.models.Model(inputs=generator_input, outputs=myDiscOutput)\n",
    "\n",
    "myDisc.trainable = False\n",
    "\n",
    "# Compile the GAN\n",
    "myBigModel.compile(optimizer=keras.optimizers.Adam(lr=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['mean_squared_error', 'mean_absolute_error','binary_crossentropy'])\n",
    "#myDisc.trainable = True\n",
    "#inputs = np.random.random((len(connectivities_padded),))\n",
    "\n",
    "\n",
    "# Input the training data\n",
    "inputs = [matrices_padded, headers_padded, connectivities_padded]\n",
    "\n",
    "def train_discriminator(D, G, noise_in):\n",
    "    fakeInputs = G.predict(noise_in)\n",
    "    allInputs = [np.vstack((i,j)) for i,j in zip(inputs, fakeInputs)]\n",
    "    \n",
    "    allOutputs = [1]*len(inputs[0]) + [0]*len(fakeInputs[0])\n",
    "    inputObjects = zip(*allInputs)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(inputObjects, allOutputs, test_size=0.33)\n",
    "\n",
    "    D.fit(map(list,zip(*X_train)),y_train, epochs=1, verbose=True)\n",
    "\n",
    "# Train the GAN\n",
    "for j in range(10):\n",
    "    print \"Iteration:\", j\n",
    "    noise_input = np.random.normal(0,1,(len(inputs[0]),15))\n",
    "    train_discriminator(myDisc,myGen,noise_input)\n",
    "    myBigModel.fit(noise_input, [1]*len(noise_input), epochs=1, verbose=True)\n",
    "print \"done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gen_onehot_dict_default(headers_padded)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mols3]",
   "language": "python",
   "name": "conda-env-mols3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
